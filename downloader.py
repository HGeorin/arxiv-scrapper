import os
import requests
import time
# import tqdm
import logging
from pymongo import MongoClient

logging.basicConfig(filename='./logs/cs-downloader.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# è¿æ¥åˆ°å¯ç”¨è®¤è¯çš„ MongoDB
username = 'vector4d'
pwd ='4dv999'
ip = 'e102.extrotec.com'
port = '32217'
client = MongoClient(
    f'mongodb://{username}:{pwd}@{ip}:{port}/?authSource=admin'
)

# é€‰æ‹©æ•°æ®åº“å’Œé›†åˆ
db = client['arxiv']
collection = db['cs-paper']

# æŸ¥è¯¢éœ€è¦ä¸‹è½½çš„è®ºæ–‡
papers = collection.find({"download_mark": 0, "main_theme": 'cs.CV'}, {"arxivId": 1, "file_path": 1})

# ä¸‹è½½ç›®å½•
BASE_DIR = "./paper_storage"

print("downloading files. Press Ctrl + C to cancel ...")

# ä¸‹è½½è¶…æ—¶æ—¶é—´
max_time = 12 * 60  # æœ€å¤§ä¸‹è½½æ—¶é—´

def download_one_paper():
    # ä¸‹è½½ PDF
    response = requests.get(pdf_url, stream=True, timeout=max_time)
    if response.status_code == 200:
        download_flag = 1
        with open(full_path, "wb") as pdf_file:
            start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
            for chunk in response.iter_content(chunk_size=1024):
                # æ£€æŸ¥ä¸‹è½½æ€»æ—¶é•¿
                elapsed_time = time.time() - start_time
                if elapsed_time > max_time:  # å¦‚æœä¸‹è½½æ—¶é—´è¶…è¿‡æœ€å¤§é™åˆ¶
                    download_flag = 0
                    break
                pdf_file.write(chunk)
            
        # ä¸‹è½½æˆåŠŸï¼Œæ›´æ–°æ•°æ®åº“
        if download_flag:
            collection.update_one({"arxivId": arxiv_id}, {"$set": {"download_mark": 1}})
            logging.info(f"âœ… download successfully : {full_path}")
        else:
            os.remove(full_path)
            logging.warning(f"âš ï¸ download timeout : {arxiv_id}")
    else:
        #è¿”å›404çŠ¶æ€ç ï¼Œå¯èƒ½æ²¡æœ‰pdfä¸‹è½½æ–¹å¼ï¼Œæ ‡è®°æˆdownload_mark=2
        if response.status_code == 404:
            collection.update_one({"arxivId": arxiv_id}, {"$set": {"download_mark": 2}})
        logging.warning(f"âš ï¸ response failed : {arxiv_id}ï¼Œstatus: {response.status_code}")

for paper in papers:
    arxiv_id = paper["arxivId"]
    file_path = paper["file_path"]

    # è®ºæ–‡ PDF ä¸‹è½½ URL
    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
    
    # è®ºæ–‡æœ¬åœ°å­˜å‚¨è·¯å¾„
    full_path = os.path.join(BASE_DIR, file_path)

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(full_path), exist_ok=True)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œè‹¥å­˜åœ¨åˆ™åˆ é™¤
    if os.path.exists(full_path):
        os.remove(full_path)

    try:
        download_one_paper()
    except requests.exceptions.Timeout:
        os.remove(full_path)
        logging.warning(f"âš ï¸ connection timeout and skip : {arxiv_id}")
    except Exception as e:
        logging.error(f"âŒ {arxiv_id}, {e}")
    time.sleep(5)

logging.info("ğŸ“‚ all the papers have been downloaded ")